"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[5674],{28453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>d});var i=s(96540);const r={},l=i.createContext(r);function t(e){const n=i.useContext(l);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(l.Provider,{value:n},e.children)}},28594:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"system-architecture/Class-diagrams","title":"Class Diagrams","description":"Figure 1. High level view Front end class diagram","source":"@site/docs/system-architecture/Class-diagrams.md","sourceDirName":"system-architecture","slug":"/system-architecture/Class-diagrams","permalink":"/project-002-highlighting/docs/system-architecture/Class-diagrams","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-002-highlighting/edit/main/documentation/docs/system-architecture/Class-diagrams.md","tags":[],"version":"current","lastUpdatedBy":"Jude","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docsSidebar","previous":{"title":"Development Environment","permalink":"/project-002-highlighting/docs/system-architecture/development-environment"},"next":{"title":"Sequence-Diagrams","permalink":"/project-002-highlighting/docs/system-architecture/Sequence-Diagrams"}}');var r=s(74848),l=s(28453);const t={sidebar_position:5},d="Class Diagrams",c={},o=[{value:"Frontend Components",id:"frontend-components",level:3},{value:"1. Audio Transcription Server",id:"1-audio-transcription-server",level:4},{value:"2. Tiles Component",id:"2-tiles-component",level:4},{value:"3. State Management Providers",id:"3-state-management-providers",level:4},{value:"Backend Components",id:"backend-components",level:2},{value:"1. Audio Transcription Server",id:"1-audio-transcription-server-1",level:4},{value:"2. Tile Prediction System",id:"2-tile-prediction-system",level:4},{value:"3. Python FastAPI Server",id:"3-python-fastapi-server",level:4},{value:"Data Flow",id:"data-flow",level:4},{value:"Audio Transcription Flow",id:"audio-transcription-flow",level:4},{value:"Tile Prediction Flow",id:"tile-prediction-flow",level:4},{value:"Highlighting Flow",id:"highlighting-flow",level:4},{value:"Model Details",id:"model-details",level:2},{value:"Whisper Model",id:"whisper-model",level:4},{value:"Embedding Model",id:"embedding-model",level:4},{value:"LLM Model",id:"llm-model",level:4},{value:"Configuration",id:"configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:4},{value:"Server Configuration",id:"server-configuration",level:4},{value:"Audio Processing Settings",id:"audio-processing-settings",level:4}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"class-diagrams",children:"Class Diagrams"})}),"\n",(0,r.jsx)("img",{width:"1724",height:"643",alt:"FE_classdiagram",src:"https://github.com/user-attachments/assets/4b3d35cf-fabc-42f9-b2b7-c452a4f87620"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure 1."})," High level view Front end class diagram"]}),"\n",(0,r.jsx)(n.h3,{id:"frontend-components",children:"Frontend Components"}),"\n",(0,r.jsx)(n.h4,{id:"1-audio-transcription-server",children:"1. Audio Transcription Server"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"backend/server.mjs"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Handles real-time audio transcription using local Whisper model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Client (WebSocket)\n    \u2193\nAudio Chunks (WebM)\n    \u2193\nFFmpeg (WebM \u2192 PCM)\n    \u2193\nWAV File Creation\n    \u2193\nWhisper Model (Local)\n    \u2193\nTranscription Text\n    \u2193\nClient (WebSocket)\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-tiles-component",children:"2. Tiles Component"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"frontend/src/components/AAC/Tiles.tsx"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Displays the AAC board with tiles and applies highlighting based on predictions."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Features"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Renders tiles in a grid layout"}),"\n",(0,r.jsx)(n.li,{children:"Supports hierarchical navigation (folders/subtiles)"}),"\n",(0,r.jsx)(n.li,{children:"Applies opacity-based highlighting to predicted tiles"}),"\n",(0,r.jsx)(n.li,{children:"Supports multiple highlight methods (opacity, darken)"}),"\n",(0,r.jsx)(n.li,{children:"Manages tile selection and navigation state"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Highlighting Logic"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Predicted tiles: 100% opacity, pulsing animation when selected, border outline when selected\n// Non-predicted tiles: 50% opacity (when predictions exist), not pulsing, doesn't have an outline\n// Default: 100% opacity (when no predictions)\n\nconst tileOpacity = shouldBeHighlighted ? 100 : 50;\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"State Management"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Uses ",(0,r.jsx)(n.code,{children:"useTilesProvider"})," for tile data"]}),"\n",(0,r.jsxs)(n.li,{children:["Uses ",(0,r.jsx)(n.code,{children:"usePredictedTiles"})," for prediction results"]}),"\n",(0,r.jsxs)(n.li,{children:["Uses ",(0,r.jsx)(n.code,{children:"useHighlightMethods"})," for highlight mode selection"]}),"\n",(0,r.jsxs)(n.li,{children:["Uses reducer (",(0,r.jsx)(n.code,{children:"stackReducer"}),") for navigation stack"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-state-management-providers",children:"3. State Management Providers"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"frontend/src/react-state-management/providers/"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Providers"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"TranscriptProvider"}),": Manages transcript state"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Stores accumulated transcript text"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.code,{children:"transcript"})," and ",(0,r.jsx)(n.code,{children:"setTranscript"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PredictedTilesProvider"}),": Manages predicted tiles"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Stores array of predicted tile words"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.code,{children:"predictedTiles"})," and ",(0,r.jsx)(n.code,{children:"setPredictedTiles"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RecordingControlProvider"}),": Manages recording state"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tracks if recording is active"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.code,{children:"isActive"})," and ",(0,r.jsx)(n.code,{children:"setIsActive"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"useUtteredTiles"}),": Tracks pressed tiles"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Maintains history of clicked tiles"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.code,{children:"tiles"}),", ",(0,r.jsx)(n.code,{children:"addTile"}),", ",(0,r.jsx)(n.code,{children:"removeLastTile"}),", ",(0,r.jsx)(n.code,{children:"clear"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"HighlightMethodsProvider"}),": Manages highlight modes"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Supports 'opacity' and 'darken' methods"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.code,{children:"activeHighlights"})," Set"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)("img",{width:"1579",height:"579",alt:"BE_classdiagram",src:"https://github.com/user-attachments/assets/bdf9d157-193a-4df9-817f-81279c72c10c"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure 2."})," High level view Back end class diagram"]}),"\n",(0,r.jsx)(n.h2,{id:"backend-components",children:"Backend Components"}),"\n",(0,r.jsx)(n.h4,{id:"1-audio-transcription-server-1",children:"1. Audio Transcription Server"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"backend/server.mjs"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Handles real-time audio transcription using local Whisper model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Client (WebSocket)\n    \u2193\nAudio Chunks (WebM)\n    \u2193\nFFmpeg (WebM \u2192 PCM)\n    \u2193\nWAV File Creation\n    \u2193\nWhisper Model (Local)\n    \u2193\nTranscription Text\n    \u2193\nClient (WebSocket)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Functions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audio Processing Pipeline"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:'// Receives WebM audio chunks\nsocket.on("audio-chunk", (data) => {\n  ffmpeg.stdin.write(Buffer.from(data))\n})\n\n// FFmpeg converts to PCM\nffmpeg.stdout.on("data", (chunk) => {\n  audioBuffer = Buffer.concat([audioBuffer, chunk])\n})\n\n// Process audio every 3-6 seconds\nconst processAudio = async () => {\n  const wavData = createWavFile(pcmChunk)\n  const transcribedText = await transcribeAudioLocal(filePath)\n  socket.emit("transcript", transcribedText)\n}\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper Transcription"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Model: ",(0,r.jsx)(n.code,{children:"Xenova/whisper-small.en"})]}),"\n",(0,r.jsx)(n.li,{children:"Input: WAV file (16kHz, mono, PCM)"}),"\n",(0,r.jsx)(n.li,{children:"Output: Transcribed text"}),"\n",(0,r.jsxs)(n.li,{children:["Anti-hallucination parameters:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"temperature: 0"})," (greedy decoding)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"no_speech_threshold: 0.6"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"logprob_threshold: -1.0"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"compression_ratio_threshold: 2.4"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Audio Validation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Checks RMS energy to detect silence"}),"\n",(0,r.jsx)(n.li,{children:"Validates minimum audio duration (1.5 seconds)"}),"\n",(0,r.jsx)(n.li,{children:"Filters out hallucinations using pattern matching"}),"\n",(0,r.jsx)(n.li,{children:"Removes unwanted markers from transcription"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duplicate Detection"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Compares new transcriptions with previous ones"}),"\n",(0,r.jsx)(n.li,{children:"Uses similarity scoring to prevent duplicate sends"}),"\n",(0,r.jsx)(n.li,{children:"Implements time-based throttling (2 seconds minimum)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-tile-prediction-system",children:"2. Tile Prediction System"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"backend/server.mjs"})," (function: ",(0,r.jsx)(n.code,{children:"predictNextTilesLocalLLM"}),")"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Predicts relevant tiles based on transcript and pressed tiles."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Input: Transcript + Pressed Tiles\n    \u2193\nContext Embedding (all-MiniLM-L6-v2)\n    \u2193\nVector Search (Cosine Similarity)\n    \u2193\nCandidate Selection (Top 60)\n    \u2193\nLLM Prompt Generation (DistilGPT2)\n    \u2193\nText Generation\n    \u2193\nWord Extraction & Filtering\n    \u2193\nOutput: Predicted Tiles (Top 10)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prediction Modes"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transcript Only"}),": Uses only transcribed text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tiles Only"}),": Uses only recently pressed tiles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Both"}),": Combines transcript and pressed tiles"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Functions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Embedding Generation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"// Embed context (transcript + tiles)\nconst queryEmb = await embedText(combinedContext)\n\n// Embed all tile labels (cached)\nconst labelEmbeddings = await embedText(label)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Vector Search"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"// Calculate cosine similarity\nconst sims = labelEmbeddings.map(e => \n  cosineSimilarity(queryEmb, e)\n)\n\n// Get top 60 candidates\nconst topIndices = topNIndices(sims, 60)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LLM-Based Selection"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"// Generate prompt with context\nconst prompt = `Recently pressed tiles: ${pressedTiles.join(', ')}\nTranscript: \"${contextLines}\"\n\nBased on the context, select the 10 best next tiles from: ${candidateWords.join(', ')}`\n\n// Generate with LLM\nconst result = await llm(prompt, {\n  max_new_tokens: 40,\n  temperature: 0,\n  do_sample: false\n})\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Word Filtering"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Excludes common words (pronouns, prepositions, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Prioritizes high-value words (actions, emotions, nouns)"}),"\n",(0,r.jsx)(n.li,{children:"Validates words against candidate list"}),"\n",(0,r.jsx)(n.li,{children:"Returns top 10 predictions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"API Endpoint"}),": ",(0,r.jsx)(n.code,{children:"POST /api/nextTilePred"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Request Body"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "transcript": "string (optional)",\n  "pressedTiles": ["string"] (optional)\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Response"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "predictedTiles": ["word1", "word2", ...],\n  "status": "success",\n  "context": "transcript text",\n  "pressedTiles": ["tile1", "tile2", ...]\n}\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-python-fastapi-server",children:"3. Python FastAPI Server"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Location"}),": ",(0,r.jsx)(n.code,{children:"backend/src/main.py"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Provides REST API endpoints for additional features."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Endpoints"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GET /"}),": Health check"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GET /health-check"}),": Health status"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /similarity"}),": Word similarity suggestions (uses spaCy)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /tts"}),": Text-to-speech"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /rekognition"}),": Image recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /s3/*"}),": S3 file operations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /custom_tiles/*"}),": Custom tile management"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Similarity Endpoint"})," (",(0,r.jsx)(n.code,{children:"/similarity"}),"):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Uses spaCy ",(0,r.jsx)(n.code,{children:"en_core_web_lg"})," model"]}),"\n",(0,r.jsx)(n.li,{children:"Calculates semantic similarity between words"}),"\n",(0,r.jsx)(n.li,{children:"Returns top N similar words from vocabulary"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"data-flow",children:"Data Flow"}),"\n",(0,r.jsx)(n.h4,{id:"audio-transcription-flow",children:"Audio Transcription Flow"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'1. User clicks "Start Recording"\n   \u2193\n2. AudioTranscription component requests microphone access\n   \u2193\n3. MediaRecorder starts capturing audio (WebM format)\n   \u2193\n4. Audio chunks sent via WebSocket to backend\n   \u2193\n5. Backend receives chunks, converts to PCM via FFmpeg\n   \u2193\n6. Audio buffer accumulates (3-6 second chunks)\n   \u2193\n7. WAV file created from PCM data\n   \u2193\n8. Whisper model transcribes audio\n   \u2193\n9. Transcription validated and cleaned\n   \u2193\n10. Transcript sent back to client via WebSocket\n    \u2193\n11. Client updates transcript state\n    \u2193\n12. Prediction request triggered automatically\n'})}),"\n",(0,r.jsx)(n.h4,{id:"tile-prediction-flow",children:"Tile Prediction Flow"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. Transcript updated OR tiles pressed\n   \u2193\n2. AudioTranscription component calls fetchPredictions()\n   \u2193\n3. HTTP POST to /api/nextTilePred\n   Request: { transcript, pressedTiles }\n   \u2193\n4. Backend creates combined context\n   \u2193\n5. Context embedded using all-MiniLM-L6-v2\n   \u2193\n6. Vector search finds top 60 candidate tiles\n   \u2193\n7. LLM (DistilGPT2) selects best 10 from candidates\n   \u2193\n8. Response: { predictedTiles: [...] }\n   \u2193\n9. Frontend updates PredictedTilesProvider\n   \u2193\n10. Tiles component re-renders with highlighting\n"})}),"\n",(0,r.jsx)(n.h4,{id:"highlighting-flow",children:"Highlighting Flow"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"1. PredictedTilesProvider updated with new predictions\n   \u2193\n2. Tiles component receives predictedTiles array\n   \u2193\n3. For each tile:\n   - Check if tile text matches predicted tiles\n   - Check if any subtiles match (recursive)\n   \u2193\n4. Calculate opacity:\n   - Predicted: 100% opacity\n   - Non-predicted: 50% opacity (when predictions exist)\n   - Default: 100% opacity (when no predictions)\n   \u2193\n5. Apply opacity via CSS data attribute\n   \u2193\n6. Tiles visually highlighted on screen\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"model-details",children:"Model Details"}),"\n",(0,r.jsx)(n.h4,{id:"whisper-model",children:"Whisper Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": ",(0,r.jsx)(n.code,{children:"Xenova/whisper-small.en"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": ~244MB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": English only"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input"}),": 16kHz mono PCM audio"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Transcribed text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": ~1-3 seconds per 3-6 second audio chunk"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Configuration"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"{\n  return_timestamps: false,\n  language: 'en',\n  temperature: 0,  // Greedy decoding\n  no_speech_threshold: 0.6,\n  logprob_threshold: -1.0,\n  compression_ratio_threshold: 2.4\n}\n"})}),"\n",(0,r.jsx)(n.h4,{id:"embedding-model",children:"Embedding Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": ",(0,r.jsx)(n.code,{children:"Xenova/all-MiniLM-L6-v2"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": ~80MB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimensions"}),": 384"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Semantic similarity search"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": ~50-100ms per embedding"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Embeds conversation context"}),"\n",(0,r.jsx)(n.li,{children:"Embeds all tile labels (cached on startup)"}),"\n",(0,r.jsx)(n.li,{children:"Cosine similarity for vector search"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"llm-model",children:"LLM Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": ",(0,r.jsx)(n.code,{children:"Xenova/distilgpt2"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size"}),": ~350MB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Text generation for tile selection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": ~200-500ms per generation"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Configuration"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:"{\n  max_new_tokens: 40,\n  temperature: 0,  // Greedy decoding\n  do_sample: false,\n  return_full_text: false\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsx)(n.h4,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Backend"})," (",(0,r.jsx)(n.code,{children:".env"})," file):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"OPENAI_API_KEY=sk-proj-...  # For testing (not used in local mode)\n"})}),"\n",(0,r.jsx)(n.h4,{id:"server-configuration",children:"Server Configuration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ports"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Frontend: ",(0,r.jsx)(n.code,{children:"http://localhost:3000"})]}),"\n",(0,r.jsxs)(n.li,{children:["Backend (Node.js): ",(0,r.jsx)(n.code,{children:"http://localhost:5000"})]}),"\n",(0,r.jsxs)(n.li,{children:["Backend (Python FastAPI): Default port\n8\n",(0,r.jsx)(n.strong,{children:"CORS Configuration"}),":"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-javascript",children:'origins: [\n  "http://localhost:3000",\n  "http://localhost",\n  "https://highlighting.vercel.app/"\n]\n'})}),"\n",(0,r.jsx)(n.h4,{id:"audio-processing-settings",children:"Audio Processing Settings"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Rate"}),": 16000 Hz"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Channels"}),": Mono (1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format"}),": PCM 16-bit signed little-endian"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chunk Duration"}),": 3-6 seconds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Overlap"}),": 0.5 seconds (to catch boundary words)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}}}]);