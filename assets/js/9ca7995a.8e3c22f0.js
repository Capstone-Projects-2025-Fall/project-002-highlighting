"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[2987],{10415:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"testing/integration-testing","title":"Integration tests","description":"Use Case 1 - Audio transcription for hearing loss","source":"@site/docs/testing/integration-testing.md","sourceDirName":"testing","slug":"/testing/integration-testing","permalink":"/project-002-highlighting/docs/testing/integration-testing","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-002-highlighting/edit/main/documentation/docs/testing/integration-testing.md","tags":[],"version":"current","lastUpdatedBy":"Sley Chery","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Unit Testing","permalink":"/project-002-highlighting/docs/testing/unit-testing"},"next":{"title":"Acceptance test","permalink":"/project-002-highlighting/docs/testing/acceptence-testing"}}');var o=t(74848),s=t(28453);const r={sidebar_position:2},a="Integration tests",c={},l=[{value:"Use Case 1 - Audio transcription for hearing loss",id:"use-case-1---audio-transcription-for-hearing-loss",level:2},{value:"Use Case 2 - Sentence Formation",id:"use-case-2---sentence-formation",level:2},{value:"Use Case 3 - Contextual based prediction",id:"use-case-3---contextual-based-prediction",level:2},{value:"Use Case 4 - Toggling microphone when not in a conversation",id:"use-case-4---toggling-microphone-when-not-in-a-conversation",level:2}];function h(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"integration-tests",children:"Integration tests"})}),"\n",(0,o.jsx)(n.h2,{id:"use-case-1---audio-transcription-for-hearing-loss",children:"Use Case 1 - Audio transcription for hearing loss"}),"\n",(0,o.jsx)("i",{children:" User wants to communicate with a user that is hearing impaired "}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Download the app locally or visit deployed website on vercel"}),"\n",(0,o.jsx)(n.li,{children:"Toggle the microphone on for person conversing to user to speak while microphone is on and recording on board"}),"\n",(0,o.jsx)(n.li,{children:"Transcription will be shown of whats being said"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"use-case-2---sentence-formation",children:"Use Case 2 - Sentence Formation"}),"\n",(0,o.jsx)("i",{children:" Speech-impaired user wants to communicate with a user via AAC Board "}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Open application locally or through Vercel deployment"}),"\n",(0,o.jsx)(n.li,{children:"Press on tiles to form a sentence, tiles will be highlighted after based on suggested words"}),"\n",(0,o.jsx)(n.li,{children:"Click on the audio output button to translate the tiles text to speech"}),"\n",(0,o.jsx)(n.li,{children:"Board will say the tiles chosen outloud for others to hear"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"use-case-3---contextual-based-prediction",children:"Use Case 3 - Contextual based prediction"}),"\n",(0,o.jsx)("i",{children:"As a user, it is important that the device can display options to me based on what is being spoken by the other person in conversation "}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Parent or person in coversation with asks a question"}),"\n",(0,o.jsx)(n.li,{children:"AAC board picks up the question through audio input"}),"\n",(0,o.jsx)(n.li,{children:"Board formulates suggested words that could be a response"}),"\n",(0,o.jsx)(n.li,{children:"Parent/person in conversation can ask what you want for dinner and the board picks this up"}),"\n",(0,o.jsx)(n.li,{children:"Board will highlight relevant food tiles to choose from"}),"\n",(0,o.jsx)(n.li,{children:"Choose highlighted tile that best fits preference for response."}),"\n",(0,o.jsx)(n.li,{children:"Play tiles selected outloud so parent or person in conversation can hear response"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"use-case-4---toggling-microphone-when-not-in-a-conversation",children:"Use Case 4 - Toggling microphone when not in a conversation"}),"\n",(0,o.jsx)("i",{children:"As a user, it is important that I can toggle on and off the microphone when I am not in a conversation. "}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Open the application and notice that the microphone is on and actively listening for fast conversation engagement"}),"\n",(0,o.jsx)(n.li,{children:"Toggle the microphone off and on depending on if you are in a conversation or not by hitting the stop button to stop and play button to record"}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(96540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);